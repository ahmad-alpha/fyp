{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Noisy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_exts = ['jpg', 'jpeg', 'png', 'bmp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import imghdr\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "\n",
    "data_dir = '../Preprocessing/Processed_Images'\n",
    "\n",
    "for image_class in os.listdir(data_dir):\n",
    "    for image in os.listdir(os.path.join(data_dir, image_class)):\n",
    "        image_path = os.path.join(data_dir, image_class, image)\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in img_exts:\n",
    "                print(image_path)    # print the path of the image with unknown extension\n",
    "                os.remove(image_path)\n",
    "        except Exception as e:\n",
    "            print(\"Issue with image:\" .format(image_path))\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [16,32]\n",
    "for size in batch_size:\n",
    "   data = tf.keras.utils.image_dataset_from_directory('../Preprocessing/Processed_Images', batch_size=size, image_size=(256, 256), shuffle=True) #Data pipeline\n",
    "\n",
    "class_names = data.class_names #get the class names\n",
    "print(class_names)\n",
    "\n",
    "data_iterator = data.as_numpy_iterator() #allows us to access Data pipeline\n",
    "\n",
    "batch = data_iterator.next() #get the next batch of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape # shape of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1] # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "rows = 4\n",
    "cols = 8\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 12))\n",
    "fig.suptitle('Sample Images with Class Labels', fontsize=16)\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Display images in grid\n",
    "for idx, (image, label) in enumerate(zip(batch[0][:32], batch[1][:32])):\n",
    "    # Get class name from label index\n",
    "    class_name = class_names[label]\n",
    "    \n",
    "    # Display image\n",
    "    axes[idx].imshow(image.astype(\"uint8\"))\n",
    "    axes[idx].set_title(f'{class_name}', fontsize=8)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "# Add color-coded legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor=f'C{i}', markersize=10, \n",
    "                             label=name) for i, name in enumerate(class_names)]\n",
    "fig.legend(handles=legend_elements, loc='center right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)  # Make room for legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda x,y : ((x/255),y)) #normalizing the data  \n",
    "scaled_iterator = data.as_numpy_iterator() #allows us to access Data pipeline\n",
    "batch = scaled_iterator.next() #get the next batch of data\n",
    "\n",
    "batch[0].max() #max value in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * 0.7) #70% of the data for training\n",
    "val_size = int(len(data) * 0.2) #20% of the data for validation\n",
    "test_size = int(len(data) * 0.1) #10% of the data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_size, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.take(train_size) #take the first 70% of the data for training\n",
    "val = data.skip(train_size).take(val_size) #skip the first 70% and take the next 20% for validation\n",
    "test = data.skip(train_size + val_size).take(test_size) #skip the first 90% and take the next 10% for testing\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, RandomTranslation, RandomContrast, RandomCrop\n",
    "# filter size = 3x3\n",
    "# input shape = 256x256x3\n",
    "# stride = 1\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = batch[0].shape[0]\n",
    "model = Sequential() #initialize the model\n",
    "\n",
    "\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    RandomFlip(\"horizontal_and_vertical\"),         # Flip images both horizontally and vertically\n",
    "    RandomRotation(0.4),                           # Rotate images up to 40% in both directions\n",
    "    RandomZoom(height_factor=(-0.2, 0.2),          # Random zoom in/out\n",
    "               width_factor=(-0.2, 0.2)),\n",
    "    RandomTranslation(height_factor=0.2,           # Translate images up to 20% in height\n",
    "                      width_factor=0.2),           # Translate images up to 20% in width\n",
    "    RandomContrast(0.2),                           # Adjust contrast randomly\n",
    "    RandomCrop(IMAGE_SIZE - 20, IMAGE_SIZE - 20),  # Crop random parts of the image\n",
    "    tf.keras.layers.Resizing(IMAGE_SIZE, IMAGE_SIZE)  # Resize back to target size\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking what is the expected dimension order for channel\n",
    "from tensorflow.keras import backend as k\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "batch_input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "chanDim = -1\n",
    "if k.image_data_format() == \"channels_first\":\n",
    "    input_shape = (CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    batch_input_shape = (BATCH_SIZE, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    chanDim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "model_dir = '../saved_models'\n",
    "log_dir = os.path.join(model_dir, 'logs')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, 'best_model.keras'),\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        verbose=1\n",
    "    ),\n",
    "    TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        write_graph=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = os.path.join(model_dir, 'final_model.keras')\n",
    "model.save(final_model_path)\n",
    "print(f\"Model saved to {final_model_path}\")\n",
    "from matplotlib import pyplot as plt\n",
    "rows = 4\n",
    "cols = 8\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 12))\n",
    "fig.suptitle('Sample Images with Class Labels', fontsize=16)\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Display images in grid\n",
    "for idx, (image, label) in enumerate(zip(batch[0][:32], batch[1][:32])):\n",
    "    # Get class name from label index\n",
    "    class_name = class_names[label]\n",
    "    \n",
    "    # Display image\n",
    "    axes[idx].imshow(image.astype(\"uint8\"))\n",
    "    axes[idx].set_title(f'{class_name}', fontsize=8)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "# Add color-coded legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor=f'C{i}', markersize=10, \n",
    "                             label=name) for i, name in enumerate(class_names)]\n",
    "fig.legend(handles=legend_elements, loc='center right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)  # Make room for legend\n",
    "plt.show()\n",
    "\n",
    "print(train_size, val_size, test_size)\n",
    "\n",
    "train = data.take(train_size) #take the first 70% of the data for training\n",
    "val = data.skip(train_size).take(val_size) #skip the first 70% and take the next 20% for validation\n",
    "test = data.skip(train_size + val_size).take(test_size) #skip the first 90% and take the next 10% for testing\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import imghdr\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "img_exts = ['jpg', 'jpeg', 'png', 'bmp']\n",
    "data_dir = 'skinType'\n",
    "\n",
    "for image_class in os.listdir(data_dir):\n",
    "    for image in os.listdir(os.path.join(data_dir, image_class)):\n",
    "        image_path = os.path.join(data_dir, image_class, image)\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in img_exts:\n",
    "                print(image_path)    # print the path of the image with unknown extension\n",
    "                os.remove(image_path)\n",
    "        except Exception as e:\n",
    "            print(\"Issue with image:\" .format(image_path))\n",
    "batch_size = [16,32]\n",
    "for size in batch_size:\n",
    "   data = tf.keras.utils.image_dataset_from_directory('skinType', batch_size=size, image_size=(256, 256), shuffle=True) #Data pipeline\n",
    "\n",
    "class_names = data.class_names #get the class names\n",
    "print(class_names)\n",
    "\n",
    "data_iterator = data.as_numpy_iterator() #allows us to access Data pipeline\n",
    "\n",
    "batch = data_iterator.next() #get the next batch of data\n",
    "\n",
    "batch[0].shape # shape of the batch\n",
    "       \n",
    "batch[1] # labels\n",
    "\n",
    "data = data.map(lambda x,y : ((x/255),y)) #normalizing the data  \n",
    "scaled_iterator = data.as_numpy_iterator() #allows us to access Data pipeline\n",
    "batch = scaled_iterator.next() #get the next batch of data\n",
    "\n",
    "batch[0].max() #max value in the batch\n",
    "\n",
    "len(data)\n",
    "\n",
    "train_size = int(len(data) * 0.7) #70% of the data for training\n",
    "val_size = int(len(data) * 0.2) #20% of the data for validation\n",
    "test_size = int(len(data) * 0.1) #10% of the data for testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from vit_pytorch import ViT\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, mean_squared_error\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lamb import Lamb\n",
    "from torch.optim import AdamW\n",
    "import timm \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#unprocessed Data\n",
    "# dataset_path = '../skinType' \n",
    "\n",
    "#preprocessed Data\n",
    "dataset_path = '../Preprocessing/Processed_Images'\n",
    "#dataset_path=\"../skintypepatches 128x128\"\n",
    "#_FaceCrops_256' \n",
    "\n",
    "# Define hyperparameters\n",
    "batch_sizes = [ 8, 32,64]\n",
    "learning_rates =[0.01, 0.001]\n",
    "optimizers_list = ['LAMB', 'AdamW', 'SGD', 'RAdam']\n",
    "\n",
    "num_classes = 3  # Dry, Normal, Oily skin types\n",
    "#class_names = ['oily', 'dry', 'normal']\n",
    "steps_per_epoch = 20 \n",
    "total_epochs=500\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "# Load Dataset\n",
    "transform = transforms.Compose([\n",
    "      transforms.Resize((224, 224)),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(10),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean, std)\n",
    "# ])\n",
    "\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean, std)\n",
    "# ])\n",
    "# Dataset Path\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.20 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "#val_dataset.dataset.transform = val_transform\n",
    "print(f\"train size {train_size}\")\n",
    "print(f\"Val size {val_size}\")\n",
    "print(f\"Test Size {test_size}\")\n",
    "\n",
    "# print(f\"train Dataset {train_dataset}\")\n",
    "# print(f\"Val Dataset {val_dataset}\")\n",
    "# print(f\"Test Dataset {test_dataset}\")\n",
    "\n",
    "# Function to initialize the model\n",
    "# def create_vit_model():    \n",
    "#     model = ViT(\n",
    "#         image_size=128,\n",
    "#         patch_size=16,\n",
    "#         num_classes=num_classes,  \n",
    "#         dim=512,                      \n",
    "#         heads=8,  \n",
    "#         depth=8,              \n",
    "#         mlp_dim=512,          \n",
    "#         dropout=0.4,           \n",
    "#         emb_dropout=0.4        \n",
    "#     )\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# Optimizer choices\n",
    "def get_optimizer(optimizer_name, model_params, lr,weight_decay=0.001):\n",
    "    if optimizer_name == 'AdamW':\n",
    "        return optim.AdamW(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'LAMB':\n",
    "        return Lamb(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        return SGD(model_params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'RAdam':\n",
    "        return RAdam(model_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    y_true, y_pred = [], []\n",
    "    all_probs = []\n",
    "\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.detach().cpu().numpy())\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    try:\n",
    "        unique_classes = np.unique(all_labels)\n",
    "        if len(unique_classes) == num_classes:\n",
    "            roc_auc = roc_auc_score(all_labels, np.array(all_probs), multi_class='ovr', average='weighted')\n",
    "        else:\n",
    "            roc_auc = None\n",
    "    except ValueError:\n",
    "        roc_auc = None \n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n",
    "\n",
    "    return running_loss / len(loader), accuracy, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Validation and Test Function\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    all_probs = [] \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    try:\n",
    "       \n",
    "        unique_classes = np.unique(all_labels)\n",
    "        if len(unique_classes) == num_classes:\n",
    "            roc_auc = roc_auc_score(all_labels, np.array(all_probs), multi_class='ovr', average='weighted')\n",
    "        else:\n",
    "            print(\"Warning: Not all classes are present in the evaluation set. Skipping ROC-AUC calculation.\")\n",
    "            roc_auc = None\n",
    "    except ValueError:\n",
    "        roc_auc = None  \n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n",
    "    \n",
    "    return running_loss / len(loader), accuracy, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "def train_and_evaluate(batch_size, lr, optimizer_name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
    "    model.to(device)\n",
    "    optimizer = get_optimizer(optimizer_name, model.parameters(), lr)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    # Training\n",
    "    for epoch in range(100,total_epochs,20):\n",
    "        # print(\"\\n\\n-------------------------Checking Weights in Each Iteration----------------------------\")\n",
    "        # print(\"Initial Weights:\")\n",
    "        # print_weights(model)\n",
    "        # print(\"\\n\\n-------------------------Checking Weights in Each Iteration-----------------------------\")\n",
    "  \n",
    "        train_loss, train_acc, train_precision, train_recall, train_f1, train_roc_auc, train_rmse = train_model(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        val_loss, val_acc, val_precision, val_recall, val_f1, val_roc_auc, val_rmse= evaluate_model(model, val_loader, criterion, device)\n",
    "        test_loss, test_acc, test_precision, test_recall, test_f1, test_roc_auc, test_rmse = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step()\n",
    "        scheduler.step(val_loss)\n",
    "        print(\"----------Values After Training-----------\")\n",
    "        print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\",\n",
    "              f\"\\nOptimizer: {optimizer_name} \\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                   f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train  F1: {train_f1:.4f},\"\n",
    "                   f\"Train ROC AUC: {train_roc_auc:.4f}, Train RMSE: {train_rmse:.4f}\")\n",
    "        \n",
    "        print(\"\\n\\n-----------Values After Validation-----------\")\n",
    "        print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "              f\"\\nOptimizer: {optimizer_name} \\nVal Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "                    f\"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}, Val ROC AUC: {val_roc_auc:.4f}, \"\n",
    "                    f\"Val RMSE: {val_rmse:.4f}\")\n",
    "        \n",
    "        print(\"\\n\\n-----------Values After Testing-----------\")\n",
    "        print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "              f\"\\nOptimizer: {optimizer_name} Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, \"\n",
    "                    f\"Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}, Test ROC AUC: {test_roc_auc:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "                \n",
    "        \n",
    "       \n",
    "        \n",
    "        #Saving Training data\n",
    "        overall_result = {\n",
    "            'Epoch': epoch + 1,\n",
    "            'Batch Size': batch_size,\n",
    "            'Learning Rate': lr,\n",
    "            'Optimizer': optimizer_name,\n",
    "            \n",
    "            'Train Loss':round(train_loss, 4),\n",
    "            'Test Loss':round(test_loss, 4),\n",
    "            'Val Loss':round(val_loss, 4),\n",
    "            \n",
    "            'Train Acc': round(train_acc, 4),\n",
    "            'Test Acc': round(test_acc, 4),\n",
    "            'Val Acc': round(val_acc, 4),\n",
    "            \n",
    "            \n",
    "            'Train Precision': round(train_precision, 4),\n",
    "            'Test Precision': round(test_precision, 4),\n",
    "            'Val Precision': round(val_precision, 4),\n",
    "            \n",
    "            'Train Recall': round(train_recall, 4),\n",
    "            'Test Recall': round(test_recall, 4),\n",
    "            'Val Recall': round(val_recall, 4),\n",
    "            \n",
    "            'Train F1 Score': round(train_f1, 4),\n",
    "            'Test F1 Score': round(test_f1, 4),\n",
    "            'Val F1 Score': round(val_f1, 4),\n",
    "            \n",
    "            'Train ROC AUC': round(train_roc_auc, 4) if train_roc_auc is not None else None,\n",
    "            'Test ROC AUC': round(test_roc_auc, 4) if test_roc_auc is not None else None,\n",
    "            'Val ROC AUC': round(val_roc_auc, 4) if val_roc_auc is not None else None,\n",
    "            \n",
    "            'Train RMSE': round(train_rmse, 4),\n",
    "            'Test RMSE': round(test_rmse, 4),\n",
    "            'Val RMSE': round(val_rmse, 4)\n",
    "        }\n",
    "        \n",
    "        # Append to CSV\n",
    "        overall_result_file = 'pretrained_overall_result.csv'\n",
    "        \n",
    "        if not os.path.isfile(overall_result_file):\n",
    "            pd.DataFrame([overall_result]).to_csv(overall_result_file, index=False)        \n",
    "        else:\n",
    "            pd.DataFrame([overall_result]).to_csv(overall_result_file, mode='a', index=False, header=False)\n",
    "            \n",
    "    # Save the trained model\n",
    "    os.makedirs('../saved_models', exist_ok=True)\n",
    "    model_save_path = f\"../saved_models/vit_model_bs{batch_size}_lr{lr}_optimizer{optimizer_name}.pth\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    # # Convert results into DataFrame\n",
    "    df_results = pd.read_csv('pretrained_overall_result.csv')\n",
    "\n",
    "    # Reshape the dataframe to long format for seaborn\n",
    "    df_long = pd.melt(df_results, id_vars=['Epoch'], var_name='Metric', value_name='Value')\n",
    "\n",
    "    # Optional: Drop rows with NaN values in the Value column (if any)\n",
    "    df_long = df_long.dropna(subset=['Value'])\n",
    "\n",
    "    # Set up the plots with a style\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Plot Loss (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_long[df_long['Metric'].isin(['Train Loss', 'Test Loss', 'Val Loss'])],x='Epoch', y='Value', hue='Metric')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Accuracy (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_long[df_long['Metric'].isin(['Train Acc', 'Test Acc', 'Val Acc'])],\n",
    "                x='Epoch', y='Value', hue='Metric')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Precision (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_long[df_long['Metric'].isin(['Train Precision', 'Test Precision', 'Val Precision'])],x='Epoch', y='Value', \n",
    "                 hue='Metric')\n",
    "    plt.title('Precision Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Recall (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_long[df_long['Metric'].isin(['Train Recall', 'Test Recall', 'Val Recall'])],x='Epoch', y='Value', hue='Metric')\n",
    "    plt.title('Recall Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot F1 Score (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_long[df_long['Metric'].isin(['Train F1 Score', 'Test F1 Score', 'Val F1 Score'])],\n",
    "                x='Epoch', y='Value', hue='Metric')\n",
    "    plt.title('F1 Score Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC AUC (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_long[df_long['Metric'].isin(['Train ROC AUC', 'Test ROC AUC', 'Val ROC AUC'])],\n",
    "                x='Epoch', y='Value', hue='Metric')\n",
    "    plt.title('ROC AUC Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('ROC AUC')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot RMSE (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_long[df_long['Metric'].isin(['Train RMSE', 'Test RMSE', 'Val RMSE'])],\n",
    "                x='Epoch', y='Value', hue='Metric')\n",
    "    plt.title('RMSE Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.show()\n",
    "\n",
    "    # return test_acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for optimizer_name in optimizers_list:\n",
    "                #test_metrics = \n",
    "                train_and_evaluate(batch_size, lr, optimizer_name)\n",
    "                # print(f\"Final Test Metrics with Batch Size {batch_size}, LR {lr}, Optimizer {optimizer_name}: {test_metrics}\")\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Model With Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, root_mean_squared_error\n",
    "from torch_optimizer import Lamb\n",
    "import clip\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_sizes = [8]\n",
    "learning_rates = [0.01, 0.001]\n",
    "optimizers_list = ['LAMB', 'AdamW']\n",
    "\n",
    "# batch_sizes = [8, 32, 64]\n",
    "# learning_rates = [0.01, 0.001]\n",
    "# optimizers_list = ['LAMB', 'AdamW']\n",
    "\n",
    "total_epochs = 250\n",
    "start=1\n",
    "step=1\n",
    "\n",
    "# Load CLIP model and preprocessing\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                         (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "data_dir = r\"../skintypepatches 128x128\"\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "print(\"Classes:\", dataset.classes)\n",
    "\n",
    "# Data Splitting\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "print(f\"train size {train_size}\")\n",
    "print(f\"Val size {val_size}\")\n",
    "print(f\"Test Size {test_size}\")\n",
    "\n",
    "def get_optimizer(optimizer_name, model_params, lr,weight_decay=0.001):\n",
    "    if optimizer_name == 'AdamW':\n",
    "        return optim.AdamW(model_params, lr=lr,weight_decay=weight_decay)\n",
    "    elif optimizer_name=='LAMB':\n",
    "        return Lamb(model_params, lr=lr,weight_decay=weight_decay)\n",
    "    \n",
    "    \n",
    "# Freeze CLIP vision encoder\n",
    "for param in clip_model.visual.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Model definition\n",
    "class CLIPSkinClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=3):\n",
    "        super(CLIPSkinClassifier, self).__init__()\n",
    "        self.encoder = clip_model.visual\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "model = CLIPSkinClassifier(clip_model, num_classes=3).to(device).float()\n",
    "\n",
    "\n",
    "\n",
    "# Metric helper\n",
    "def compute_metrics(outputs, labels):\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(labels, preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(np.eye(3)[labels], F.softmax(outputs, dim=1).cpu().detach().numpy(), multi_class='ovr')\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    rmse = root_mean_squared_error(labels, preds)\n",
    "    return acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Train loop\n",
    "def train_model(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in tqdm(loader):\n",
    "        images, labels = images.to(device).float(), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_outputs.append(outputs.detach())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    outputs_cat = torch.cat(all_outputs)\n",
    "    labels_cat = torch.cat(all_labels)\n",
    "    acc, precision, recall, f1, roc_auc, rmse = compute_metrics(outputs_cat, labels_cat)\n",
    "    return total_loss / len(loader), acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Eval loop\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader):\n",
    "            images, labels = images.to(device).float(), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_outputs.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    outputs_cat = torch.cat(all_outputs)\n",
    "    labels_cat = torch.cat(all_labels)\n",
    "    acc, precision, recall, f1, roc_auc, rmse = compute_metrics(outputs_cat, labels_cat)\n",
    "    return total_loss / len(loader), acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Training loop with logging\n",
    "\n",
    "\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for optimizer_name in optimizers_list:\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            # Loss and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = Lamb(model.parameters(), lr=lr)\n",
    "            \n",
    "            for epoch in range(start,total_epochs,step):\n",
    "                train_loss, train_acc, train_precision, train_recall, train_f1, train_roc_auc, train_rmse = train_model(model, train_loader, optimizer, criterion, device)\n",
    "                val_loss, val_acc, val_precision, val_recall, val_f1, val_roc_auc, val_rmse = evaluate_model(model, val_loader, criterion, device)\n",
    "                test_loss, test_acc, test_precision, test_recall, test_f1, test_roc_auc, test_rmse = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "                print(\"----------Values After Training-----------\")\n",
    "                print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\",\n",
    "                    f\"\\nOptimizer: {optimizer_name} \\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                        f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train  F1: {train_f1:.4f},\"\n",
    "                        f\"Train ROC AUC: {train_roc_auc:.4f}, Train RMSE: {train_rmse:.4f}\")\n",
    "\n",
    "                print(\"\\n\\n-----------Values After Validation-----------\")\n",
    "                print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "                    f\"\\nOptimizer: {optimizer_name} \\nVal Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "                            f\"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}, Val ROC AUC: {val_roc_auc:.4f}, \"\n",
    "                            f\"Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "                print(\"\\n\\n-----------Values After Testing-----------\")\n",
    "                print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "                    f\"\\nOptimizer: {optimizer_name} Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, \"\n",
    "                            f\"Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}, Test ROC AUC: {test_roc_auc:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "                # Save results to CSV\n",
    "                overall_result = {\n",
    "                    'Epoch': epoch + 1,\n",
    "                    'Batch Size': batch_size,\n",
    "                    'Learning Rate': lr,\n",
    "                    'Optimizer': optimizer_name,\n",
    "\n",
    "                    'Train Loss': round(train_loss, 4),\n",
    "                    'Test Loss': round(test_loss, 4),\n",
    "                    'Val Loss': round(val_loss, 4),\n",
    "\n",
    "                    'Train Acc': round(train_acc, 4),\n",
    "                    'Test Acc': round(test_acc, 4),\n",
    "                    'Val Acc': round(val_acc, 4),\n",
    "\n",
    "                    'Train Precision': round(train_precision, 4),\n",
    "                    'Test Precision': round(test_precision, 4),\n",
    "                    'Val Precision': round(val_precision, 4),\n",
    "\n",
    "                    'Train Recall': round(train_recall, 4),\n",
    "                    'Test Recall': round(test_recall, 4),\n",
    "                    'Val Recall': round(val_recall, 4),\n",
    "\n",
    "                    'Train F1 Score': round(train_f1, 4),\n",
    "                    'Test F1 Score': round(test_f1, 4),\n",
    "                    'Val F1 Score': round(val_f1, 4),\n",
    "\n",
    "                    'Train ROC AUC': round(train_roc_auc, 4) if train_roc_auc is not None else None,\n",
    "                    'Test ROC AUC': round(test_roc_auc, 4) if test_roc_auc is not None else None,\n",
    "                    'Val ROC AUC': round(val_roc_auc, 4) if val_roc_auc is not None else None,\n",
    "\n",
    "                    'Train RMSE': round(train_rmse, 4),\n",
    "                    'Test RMSE': round(test_rmse, 4),\n",
    "                    'Val RMSE': round(val_rmse, 4)\n",
    "                }\n",
    "\n",
    "                overall_result_file = 'sample result (250).csv'\n",
    "                if not os.path.isfile(overall_result_file):\n",
    "                    pd.DataFrame([overall_result]).to_csv(overall_result_file, index=False)\n",
    "                else:\n",
    "                    pd.DataFrame([overall_result]).to_csv(overall_result_file, mode='a', index=False, header=False)\n",
    "\n",
    "            # Save model\n",
    "            os.makedirs('../saved_models', exist_ok=True)\n",
    "            model_save_path = f\"../saved_models/vit_model_bs{batch_size}_lr{lr}_optimizer{optimizer_name}.pth\"\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Model With Clipping (ahmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, root_mean_squared_error\n",
    "from torch_optimizer import Lamb\n",
    "import clip\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_sizes = [8]\n",
    "learning_rates = [0.01, 0.001]\n",
    "optimizers_list = ['LAMB', 'AdamW']\n",
    "total_epochs = 500\n",
    "start=100\n",
    "step=20\n",
    "\n",
    "# Load CLIP model and preprocessing\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                         (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "data_dir = r\"../skintypepatches 128x128\"\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "print(\"Classes:\", dataset.classes)\n",
    "\n",
    "# Data Splitting\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"train size {train_size}\")\n",
    "print(f\"Val size {val_size}\")\n",
    "print(f\"Test Size {test_size}\")\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer_name, model_params, lr,weight_decay=0.001):\n",
    "    if optimizer_name == 'AdamW':\n",
    "        return optim.AdamW(model_params, lr=lr,weight_decay=weight_decay)\n",
    "    elif optimizer_name=='LAMB':\n",
    "        return Lamb(model_params, lr=lr,weight_decay=weight_decay)\n",
    "    \n",
    "    \n",
    "# Freeze CLIP vision encoder\n",
    "for param in clip_model.visual.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Model definition\n",
    "class CLIPSkinClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=3):\n",
    "        super(CLIPSkinClassifier, self).__init__()\n",
    "        self.encoder = clip_model.visual\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "model = CLIPSkinClassifier(clip_model, num_classes=3).to(device).float()\n",
    "\n",
    "\n",
    "\n",
    "# Metric helper\n",
    "def compute_metrics(outputs, labels):\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(labels, preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(np.eye(3)[labels], F.softmax(outputs, dim=1).cpu().detach().numpy(), multi_class='ovr')\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    rmse = root_mean_squared_error(labels, preds)\n",
    "    return acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Train loop\n",
    "def train_model(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in tqdm(loader):\n",
    "        images, labels = images.to(device).float(), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_outputs.append(outputs.detach())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    outputs_cat = torch.cat(all_outputs)\n",
    "    labels_cat = torch.cat(all_labels)\n",
    "    acc, precision, recall, f1, roc_auc, rmse = compute_metrics(outputs_cat, labels_cat)\n",
    "    return total_loss / len(loader), acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Eval loop\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader):\n",
    "            images, labels = images.to(device).float(), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_outputs.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    outputs_cat = torch.cat(all_outputs)\n",
    "    labels_cat = torch.cat(all_labels)\n",
    "    acc, precision, recall, f1, roc_auc, rmse = compute_metrics(outputs_cat, labels_cat)\n",
    "    return total_loss / len(loader), acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Training loop with logging\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for optimizer_name in optimizers_list:\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            # Loss and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = Lamb(model.parameters(), lr=lr)\n",
    "            \n",
    "            for epoch in range(start,total_epochs,step):\n",
    "                train_loss, train_acc, train_precision, train_recall, train_f1, train_roc_auc, train_rmse = train_model(model, train_loader, optimizer, criterion, device)\n",
    "                val_loss, val_acc, val_precision, val_recall, val_f1, val_roc_auc, val_rmse = evaluate_model(model, val_loader, criterion, device)\n",
    "                test_loss, test_acc, test_precision, test_recall, test_f1, test_roc_auc, test_rmse = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "                print(\"----------Values After Training-----------\")\n",
    "                print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\",\n",
    "                    f\"\\nOptimizer: {optimizer_name} \\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                        f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train  F1: {train_f1:.4f},\"\n",
    "                        f\"Train ROC AUC: {train_roc_auc:.4f}, Train RMSE: {train_rmse:.4f}\")\n",
    "\n",
    "                print(\"\\n\\n-----------Values After Validation-----------\")\n",
    "                print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "                    f\"\\nOptimizer: {optimizer_name} \\nVal Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "                            f\"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}, Val ROC AUC: {val_roc_auc:.4f}, \"\n",
    "                            f\"Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "                print(\"\\n\\n-----------Values After Testing-----------\")\n",
    "                print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "                    f\"\\nOptimizer: {optimizer_name} Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, \"\n",
    "                            f\"Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}, Test ROC AUC: {test_roc_auc:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "                # Save results to CSV\n",
    "                overall_result = {\n",
    "                    'Epoch': epoch + 1,\n",
    "                    'Batch Size': batch_size,\n",
    "                    'Learning Rate': lr,\n",
    "                    'Optimizer': optimizer_name,\n",
    "\n",
    "                    'Train Loss': round(train_loss, 4),\n",
    "                    'Test Loss': round(test_loss, 4),\n",
    "                    'Val Loss': round(val_loss, 4),\n",
    "\n",
    "                    'Train Acc': round(train_acc, 4),\n",
    "                    'Test Acc': round(test_acc, 4),\n",
    "                    'Val Acc': round(val_acc, 4),\n",
    "\n",
    "                    'Train Precision': round(train_precision, 4),\n",
    "                    'Test Precision': round(test_precision, 4),\n",
    "                    'Val Precision': round(val_precision, 4),\n",
    "\n",
    "                    'Train Recall': round(train_recall, 4),\n",
    "                    'Test Recall': round(test_recall, 4),\n",
    "                    'Val Recall': round(val_recall, 4),\n",
    "\n",
    "                    'Train F1 Score': round(train_f1, 4),\n",
    "                    'Test F1 Score': round(test_f1, 4),\n",
    "                    'Val F1 Score': round(val_f1, 4),\n",
    "\n",
    "                    'Train ROC AUC': round(train_roc_auc, 4) if train_roc_auc is not None else None,\n",
    "                    'Test ROC AUC': round(test_roc_auc, 4) if test_roc_auc is not None else None,\n",
    "                    'Val ROC AUC': round(val_roc_auc, 4) if val_roc_auc is not None else None,\n",
    "\n",
    "                    'Train RMSE': round(train_rmse, 4),\n",
    "                    'Test RMSE': round(test_rmse, 4),\n",
    "                    'Val RMSE': round(val_rmse, 4)\n",
    "                }\n",
    "\n",
    "                overall_result_file = 'sample result (250).csv'\n",
    "                if not os.path.isfile(overall_result_file):\n",
    "                    pd.DataFrame([overall_result]).to_csv(overall_result_file, index=False)\n",
    "                else:\n",
    "                    pd.DataFrame([overall_result]).to_csv(overall_result_file, mode='a', index=False, header=False)\n",
    "\n",
    "            # Save model\n",
    "            model_dir = '../saved_models(pth)'\n",
    "            os.makedirs('../saved_models', exist_ok=True)\n",
    "            model_save_path = f\"../saved_models/vit_model_bs{batch_size}_lr{lr}_optimizer{optimizer_name}.pth\"\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved to {model_save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_results = pd.read_csv('reduced_200_pretrained_overall_result.csv')\n",
    "\n",
    "# Reshape the dataframe to long format for seaborn\n",
    "df_long = pd.melt(df_results, id_vars=['Epoch'], var_name='Metric', value_name='Value')\n",
    "\n",
    "# Optional: Drop rows with NaN values in the Value column (if any)\n",
    "df_long = df_long.dropna(subset=['Value'])\n",
    "\n",
    "# Set up the plots with a style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Plot Loss (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_long[df_long['Metric'].isin(['Train Loss', 'Test Loss', 'Val Loss'])],\n",
    "             x='Epoch', y='Value', hue='Metric')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(title='Metric')\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_long[df_long['Metric'].isin(['Train Acc', 'Test Acc', 'Val Acc'])],\n",
    "             x='Epoch', y='Value', hue='Metric')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(title='Metric')\n",
    "plt.show()\n",
    "\n",
    "# Plot Precision (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_long[df_long['Metric'].isin(['Train Precision', 'Test Precision', 'Val Precision'])],\n",
    "             x='Epoch', y='Value', hue='Metric')\n",
    "plt.title('Precision Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(title='Metric')\n",
    "plt.show()\n",
    "\n",
    "# Plot Recall (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_long[df_long['Metric'].isin(['Train Recall', 'Test Recall', 'Val Recall'])],\n",
    "             x='Epoch', y='Value', hue='Metric')\n",
    "plt.title('Recall Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend(title='Metric')\n",
    "plt.show()\n",
    "\n",
    "# Plot F1 Score (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_long[df_long['Metric'].isin(['Train F1 Score', 'Test F1 Score', 'Val F1 Score'])],\n",
    "             x='Epoch', y='Value', hue='Metric')\n",
    "plt.title('F1 Score Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(title='Metric')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC AUC (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_long[df_long['Metric'].isin(['Train ROC AUC', 'Test ROC AUC', 'Val ROC AUC'])],\n",
    "             x='Epoch', y='Value', hue='Metric')\n",
    "plt.title('ROC AUC Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend(title='Metric')\n",
    "plt.show()\n",
    "\n",
    "# Plot RMSE (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_long[df_long['Metric'].isin(['Train RMSE', 'Test RMSE', 'Val RMSE'])],\n",
    "             x='Epoch', y='Value', hue='Metric')\n",
    "plt.title('RMSE Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend(title='Metric')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Akhati report (12 vlaues in each graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import os\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('reduced_200_pretrained_overall_result.csv')\n",
    "\n",
    "# Set correct column names\n",
    "lr_col = 'Learning Rate'\n",
    "bs_col = 'Batch Size'\n",
    "opt_col = 'Optimizer'\n",
    "epoch_col = 'Epoch'\n",
    "\n",
    "# Metrics to plot\n",
    "metrics = [ \n",
    "    'Train Loss', 'Test Loss', 'Val Loss',\n",
    "    'Train Acc', 'Test Acc', 'Val Acc',\n",
    "    'Train Precision', 'Test Precision', 'Val Precision',\n",
    "    'Train Recall', 'Test Recall', 'Val Recall',\n",
    "    'Train F1 Score', 'Test F1 Score', 'Val F1 Score',\n",
    "    'Train ROC AUC', 'Test ROC AUC', 'Val ROC AUC',\n",
    "    'Train RMSE', 'Test RMSE', 'Val RMSE'\n",
    "]\n",
    "\n",
    "# Create unique configuration name\n",
    "df['config'] = df[lr_col].astype(str) + '_lr_' + \\\n",
    "               df[bs_col].astype(str) + '_bs_' + \\\n",
    "               df[opt_col]\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Create a directory to save images\n",
    "image_dir = \"metric_plots\"\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "# Create Word document\n",
    "doc = Document()\n",
    "doc.add_heading('Training Metrics Report', 0)\n",
    "\n",
    "# Generate plots and add to document\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for config_name, group in df.groupby('config'):\n",
    "        plt.plot(group[epoch_col], group[metric], label=config_name)\n",
    "\n",
    "    plt.title(metric)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(title='Config', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot as image\n",
    "    image_path = os.path.join(image_dir, f\"{metric.replace(' ', '_')}.png\")\n",
    "    plt.savefig(image_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Add title and image to Word doc\n",
    "    doc.add_heading(metric, level=1)\n",
    "    doc.add_picture(image_path, width=Inches(6.5))  # Adjust width if needed\n",
    "\n",
    "# Save the Word document\n",
    "doc.save('Reduced 200 Images Training Metrics Report.docx')\n",
    "print(\"Word document 'Training_Metrics_Report.docx' created with all plots.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alag alag report based on (learning rate, batch size, optimizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"reduced_200_pretrained_overall_result.csv\")\n",
    "\n",
    "# Combine hyperparameters into a config string for reference (not used in graph)\n",
    "df['Config'] = df.apply(lambda row: f\"LR={row['Learning Rate']} | BS={row['Batch Size']} | Opt={row['Optimizer']}\", axis=1)\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# List of metrics to visualize\n",
    "metrics = [\n",
    "    'Train Loss', 'Test Loss', 'Val Loss',\n",
    "    'Train Acc', 'Test Acc', 'Val Acc',\n",
    "    'Train Precision', 'Test Precision', 'Val Precision',\n",
    "    'Train Recall', 'Test Recall', 'Val Recall',\n",
    "    'Train F1 Score', 'Test F1 Score', 'Val F1 Score',\n",
    "    'Train ROC AUC', 'Test ROC AUC', 'Val ROC AUC',\n",
    "    'Train RMSE', 'Test RMSE', 'Val RMSE'\n",
    "]\n",
    "\n",
    "# Create a folder for images\n",
    "img_folder = \"metric_images\"\n",
    "os.makedirs(img_folder, exist_ok=True)\n",
    "\n",
    "# Create Word document\n",
    "doc = Document()\n",
    "doc.add_heading(\"Model Evaluation Metrics Report\", 0)\n",
    "\n",
    "# Loop over each metric, create and save plot, insert into doc\n",
    "for metric in metrics:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Plot by Learning Rate\n",
    "    sns.lineplot(data=df, x='Epoch', y=metric, hue='Learning Rate', marker='o', ax=axes[0])\n",
    "    axes[0].set_title(f\"{metric} vs Epoch (Learning Rate)\")\n",
    "    axes[0].legend(title=\"LR\", fontsize=7)\n",
    "\n",
    "    # Plot by Batch Size\n",
    "    sns.lineplot(data=df, x='Epoch', y=metric, hue='Batch Size', marker='o', ax=axes[1])\n",
    "    axes[1].set_title(f\"{metric} vs Epoch (Batch Size)\")\n",
    "    axes[1].legend(title=\"BS\", fontsize=7)\n",
    "\n",
    "    # Plot by Optimizer\n",
    "    sns.lineplot(data=df, x='Epoch', y=metric, hue='Optimizer', marker='o', ax=axes[2])\n",
    "    axes[2].set_title(f\"{metric} vs Epoch (Optimizer)\")\n",
    "    axes[2].legend(title=\"Opt\", fontsize=7)\n",
    "\n",
    "    # Save figure\n",
    "    image_path = os.path.join(img_folder, f\"{metric.replace(' ', '_')}.png\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(image_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Add to Word doc\n",
    "    doc.add_heading(metric, level=1)\n",
    "    doc.add_picture(image_path, width=Inches(6.5))  # Use full width, fits well\n",
    "\n",
    "# Save final document\n",
    "doc.save(\"Reduced 200 Images Metric Evaluation Report.docx\")\n",
    "print(\" All metric graphs saved and Word document generated as 'Metric_Evaluation_Report.docx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, root_mean_squared_error\n",
    "from torch_optimizer import Lamb\n",
    "import clip\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_sizes = [8]\n",
    "learning_rates = [0.001]\n",
    "optimizers_list = ['LAMB', 'AdamW']\n",
    "total_epochs = 500\n",
    "start=100\n",
    "step=20\n",
    "\n",
    "# Load CLIP model and preprocessing\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                         (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "data_dir = r\"../skintypepatches 128x128\"\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "print(\"Classes:\", dataset.classes)\n",
    "\n",
    "# Data Splitting\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"train size {train_size}\")\n",
    "print(f\"Val size {val_size}\")\n",
    "print(f\"Test Size {test_size}\")\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer_name, model_params, lr,weight_decay=0.001):\n",
    "    if optimizer_name == 'AdamW':\n",
    "        return optim.AdamW(model_params, lr=lr,weight_decay=weight_decay)\n",
    "    elif optimizer_name=='LAMB':\n",
    "        return Lamb(model_params, lr=lr,weight_decay=weight_decay)\n",
    "    \n",
    "    \n",
    "# Freeze CLIP vision encoder\n",
    "for param in clip_model.visual.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Model definition\n",
    "class CLIPSkinClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=3):\n",
    "        super(CLIPSkinClassifier, self).__init__()\n",
    "        self.encoder = clip_model.visual\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "model = CLIPSkinClassifier(clip_model, num_classes=3).to(device).float()\n",
    "\n",
    "\n",
    "\n",
    "# Metric helper\n",
    "def compute_metrics(outputs, labels):\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(labels, preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(np.eye(3)[labels], F.softmax(outputs, dim=1).cpu().detach().numpy(), multi_class='ovr')\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    rmse = root_mean_squared_error(labels, preds)\n",
    "    return acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Train loop\n",
    "def train_model(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in tqdm(loader):\n",
    "        images, labels = images.to(device).float(), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_outputs.append(outputs.detach())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    outputs_cat = torch.cat(all_outputs)\n",
    "    labels_cat = torch.cat(all_labels)\n",
    "    acc, precision, recall, f1, roc_auc, rmse = compute_metrics(outputs_cat, labels_cat)\n",
    "    return total_loss / len(loader), acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Eval loop\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader):\n",
    "            images, labels = images.to(device).float(), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_outputs.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    outputs_cat = torch.cat(all_outputs)\n",
    "    labels_cat = torch.cat(all_labels)\n",
    "    acc, precision, recall, f1, roc_auc, rmse = compute_metrics(outputs_cat, labels_cat)\n",
    "    return total_loss / len(loader), acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Training loop with logging\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for optimizer_name in optimizers_list:\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            # Loss and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = Lamb(model.parameters(), lr=lr)\n",
    "            \n",
    "            for epoch in range(start,total_epochs,step):\n",
    "                train_loss, train_acc, train_precision, train_recall, train_f1, train_roc_auc, train_rmse = train_model(model, train_loader, optimizer, criterion, device)\n",
    "                val_loss, val_acc, val_precision, val_recall, val_f1, val_roc_auc, val_rmse = evaluate_model(model, val_loader, criterion, device)\n",
    "                test_loss, test_acc, test_precision, test_recall, test_f1, test_roc_auc, test_rmse = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "                print(\"----------Values After Training-----------\")\n",
    "                print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\",\n",
    "                    f\"\\nOptimizer: {optimizer_name} \\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                        f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train  F1: {train_f1:.4f},\"\n",
    "                        f\"Train ROC AUC: {train_roc_auc:.4f}, Train RMSE: {train_rmse:.4f}\")\n",
    "\n",
    "                print(\"\\n\\n-----------Values After Validation-----------\")\n",
    "                print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "                    f\"\\nOptimizer: {optimizer_name} \\nVal Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "                            f\"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}, Val ROC AUC: {val_roc_auc:.4f}, \"\n",
    "                            f\"Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "                print(\"\\n\\n-----------Values After Testing-----------\")\n",
    "                print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "                    f\"\\nOptimizer: {optimizer_name} Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, \"\n",
    "                            f\"Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}, Test ROC AUC: {test_roc_auc:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "                # Save results to CSV\n",
    "                overall_result = {\n",
    "                    'Epoch': epoch + 1,\n",
    "                    'Batch Size': batch_size,\n",
    "                    'Learning Rate': lr,\n",
    "                    'Optimizer': optimizer_name,\n",
    "\n",
    "                    'Train Loss': round(train_loss, 4),\n",
    "                    'Test Loss': round(test_loss, 4),\n",
    "                    'Val Loss': round(val_loss, 4),\n",
    "\n",
    "                    'Train Acc': round(train_acc, 4),\n",
    "                    'Test Acc': round(test_acc, 4),\n",
    "                    'Val Acc': round(val_acc, 4),\n",
    "\n",
    "                    'Train Precision': round(train_precision, 4),\n",
    "                    'Test Precision': round(test_precision, 4),\n",
    "                    'Val Precision': round(val_precision, 4),\n",
    "\n",
    "                    'Train Recall': round(train_recall, 4),\n",
    "                    'Test Recall': round(test_recall, 4),\n",
    "                    'Val Recall': round(val_recall, 4),\n",
    "\n",
    "                    'Train F1 Score': round(train_f1, 4),\n",
    "                    'Test F1 Score': round(test_f1, 4),\n",
    "                    'Val F1 Score': round(val_f1, 4),\n",
    "\n",
    "                    'Train ROC AUC': round(train_roc_auc, 4) if train_roc_auc is not None else None,\n",
    "                    'Test ROC AUC': round(test_roc_auc, 4) if test_roc_auc is not None else None,\n",
    "                    'Val ROC AUC': round(val_roc_auc, 4) if val_roc_auc is not None else None,\n",
    "\n",
    "                    'Train RMSE': round(train_rmse, 4),\n",
    "                    'Test RMSE': round(test_rmse, 4),\n",
    "                    'Val RMSE': round(val_rmse, 4)\n",
    "                }\n",
    "\n",
    "                overall_result_file = 'sample result (250).csv'\n",
    "                if not os.path.isfile(overall_result_file):\n",
    "                    pd.DataFrame([overall_result]).to_csv(overall_result_file, index=False)\n",
    "                else:\n",
    "                    pd.DataFrame([overall_result]).to_csv(overall_result_file, mode='a', index=False, header=False)\n",
    "\n",
    "            # Save model\n",
    "            os.makedirs('../saved_models', exist_ok=True)\n",
    "            model_save_path = f\"../saved_models/vit_model_bs{batch_size}_lr{lr}_optimizer{optimizer_name}.pth\"\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved to {model_save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, root_mean_squared_error\n",
    "from torch_optimizer import Lamb  # You may need to install this via pip install pytorch-optimizer\n",
    "import clip\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "lr = 0.0001\n",
    "optimizer_name = \"AdamW\"\n",
    "total_epochs = 501\n",
    "start=1\n",
    "step=1\n",
    "\n",
    "# Load CLIP model and preprocessing\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                         (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "data_dir = \"../skintypepatches 128x128\"\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "print(\"Classes:\", dataset.classes)\n",
    "\n",
    "# Data Splitting\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Freeze CLIP vision encoder\n",
    "for param in clip_model.visual.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Model definition\n",
    "class CLIPSkinClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=3):\n",
    "        super(CLIPSkinClassifier, self).__init__()\n",
    "        self.encoder = clip_model.visual\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "model = CLIPSkinClassifier(clip_model, num_classes=3).to(device).float()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Lamb(model.parameters(), lr=lr)\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "# Metric helper\n",
    "def compute_metrics(outputs, labels):\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(labels, preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(np.eye(3)[labels], F.softmax(outputs, dim=1).cpu().detach().numpy(), multi_class='ovr')\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    rmse = root_mean_squared_error(labels, preds)\n",
    "    return acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Train loop\n",
    "def train_model(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in tqdm(loader):\n",
    "        images, labels = images.to(device).float(), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_outputs.append(outputs.detach())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    outputs_cat = torch.cat(all_outputs)\n",
    "    labels_cat = torch.cat(all_labels)\n",
    "    acc, precision, recall, f1, roc_auc, rmse = compute_metrics(outputs_cat, labels_cat)\n",
    "    return total_loss / len(loader), acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Eval loop\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader):\n",
    "            images, labels = images.to(device).float(), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_outputs.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    outputs_cat = torch.cat(all_outputs)\n",
    "    labels_cat = torch.cat(all_labels)\n",
    "    acc, precision, recall, f1, roc_auc, rmse = compute_metrics(outputs_cat, labels_cat)\n",
    "    return total_loss / len(loader), acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Training loop with logging\n",
    "for epoch in range(start,total_epochs,step):\n",
    "    train_loss, train_acc, train_precision, train_recall, train_f1, train_roc_auc, train_rmse = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_precision, val_recall, val_f1, val_roc_auc, val_rmse = evaluate_model(model, val_loader, criterion, device)\n",
    "    test_loss, test_acc, test_precision, test_recall, test_f1, test_roc_auc, test_rmse = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "    print(\"----------Values After Training-----------\")\n",
    "    print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\",\n",
    "          f\"\\nOptimizer: {optimizer_name} \\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "               f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train  F1: {train_f1:.4f},\"\n",
    "               f\"Train ROC AUC: {train_roc_auc:.4f}, Train RMSE: {train_rmse:.4f}\")\n",
    "\n",
    "    print(\"\\n\\n-----------Values After Validation-----------\")\n",
    "    print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "          f\"\\nOptimizer: {optimizer_name} \\nVal Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "                f\"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}, Val ROC AUC: {val_roc_auc:.4f}, \"\n",
    "                f\"Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "    print(\"\\n\\n-----------Values After Testing-----------\")\n",
    "    print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "          f\"\\nOptimizer: {optimizer_name} Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, \"\n",
    "                f\"Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}, Test ROC AUC: {test_roc_auc:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "    # Save results to CSV\n",
    "    overall_result = {\n",
    "        'Epoch': epoch + 1,\n",
    "        'Batch Size': batch_size,\n",
    "        'Learning Rate': lr,\n",
    "        'Optimizer': optimizer_name,\n",
    "\n",
    "        'Train Loss': round(train_loss, 4),\n",
    "        'Test Loss': round(test_loss, 4),\n",
    "        'Val Loss': round(val_loss, 4),\n",
    "\n",
    "        'Train Acc': round(train_acc, 4),\n",
    "        'Test Acc': round(test_acc, 4),\n",
    "        'Val Acc': round(val_acc, 4),\n",
    "\n",
    "        'Train Precision': round(train_precision, 4),\n",
    "        'Test Precision': round(test_precision, 4),\n",
    "        'Val Precision': round(val_precision, 4),\n",
    "\n",
    "        'Train Recall': round(train_recall, 4),\n",
    "        'Test Recall': round(test_recall, 4),\n",
    "        'Val Recall': round(val_recall, 4),\n",
    "\n",
    "        'Train F1 Score': round(train_f1, 4),\n",
    "        'Test F1 Score': round(test_f1, 4),\n",
    "        'Val F1 Score': round(val_f1, 4),\n",
    "\n",
    "        'Train ROC AUC': round(train_roc_auc, 4) if train_roc_auc is not None else None,\n",
    "        'Test ROC AUC': round(test_roc_auc, 4) if test_roc_auc is not None else None,\n",
    "        'Val ROC AUC': round(val_roc_auc, 4) if val_roc_auc is not None else None,\n",
    "\n",
    "        'Train RMSE': round(train_rmse, 4),\n",
    "        'Test RMSE': round(test_rmse, 4),\n",
    "        'Val RMSE': round(val_rmse, 4)\n",
    "    }\n",
    "\n",
    "    overall_result_file = 'test with shuffle.csv'\n",
    "    if not os.path.isfile(overall_result_file):\n",
    "        pd.DataFrame([overall_result]).to_csv(overall_result_file, index=False)\n",
    "    else:\n",
    "        pd.DataFrame([overall_result]).to_csv(overall_result_file, mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Classes: ['dry', 'normal', 'oily']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [02:00<00:00, 20.04it/s]\n",
      "100%|| 691/691 [00:30<00:00, 22.73it/s]\n",
      "100%|| 346/346 [00:20<00:00, 16.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.1 \n",
      "Optimizer: LAMB \n",
      "Train Loss: 1.1991, Train Acc: 0.3328, Train Precision: 0.3327, Train Recall: 0.3327, Train  F1: 0.3319,Train ROC AUC: 0.5026478630947439, Train RMSE: 1.1402\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: LAMB \n",
      "Val Loss: 1.1004, Val Acc: 0.3383, Val Precision: 0.1128, Val Recall: 0.3333, Val F1: 0.1685, Val ROC AUC: 0.5, Val RMSE: 1.2805\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: LAMB Test Loss: 1.1015, Test Acc: 0.3371, Test Precision: 0.1124, Test Recall: 0.3333, Test F1: 0.1681, Test ROC AUC: 0.5, Test RMSE: 1.2956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [01:42<00:00, 23.51it/s]\n",
      "100%|| 691/691 [00:29<00:00, 23.69it/s]\n",
      "100%|| 346/346 [00:16<00:00, 21.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.1 \n",
      "Optimizer: AdamW \n",
      "Train Loss: 1.1908, Train Acc: 0.3355, Train Precision: 0.3354, Train Recall: 0.3354, Train  F1: 0.3350,Train ROC AUC: 0.5014606040211919, Train RMSE: 1.1613\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: AdamW \n",
      "Val Loss: 1.1061, Val Acc: 0.3383, Val Precision: 0.1128, Val Recall: 0.3333, Val F1: 0.1685, Val ROC AUC: 0.5000907290427775, Val RMSE: 1.2805\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: AdamW Test Loss: 1.1067, Test Acc: 0.3367, Test Precision: 0.1123, Test Recall: 0.3330, Test F1: 0.1679, Test ROC AUC: 0.4998190744082667, Test RMSE: 1.2961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [01:55<00:00, 20.86it/s]\n",
      "100%|| 691/691 [00:41<00:00, 16.61it/s]\n",
      "100%|| 346/346 [00:19<00:00, 18.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.1 \n",
      "Optimizer: SGD \n",
      "Train Loss: 1.1221, Train Acc: 0.3297, Train Precision: 0.3297, Train Recall: 0.3297, Train  F1: 0.3297,Train ROC AUC: 0.4940071755532413, Train RMSE: 1.1613\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: SGD \n",
      "Val Loss: 1.1285, Val Acc: 0.3357, Val Precision: 0.1119, Val Recall: 0.3333, Val F1: 0.1676, Val ROC AUC: 0.4998197133716949, Val RMSE: 0.8150\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: SGD Test Loss: 1.1342, Test Acc: 0.3244, Test Precision: 0.1081, Test Recall: 0.3333, Test F1: 0.1633, Test ROC AUC: 0.5003585956736968, Test RMSE: 0.8219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [01:58<00:00, 20.40it/s]\n",
      "100%|| 691/691 [00:26<00:00, 25.85it/s]\n",
      "100%|| 346/346 [00:11<00:00, 29.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.1 \n",
      "Optimizer: RAdam \n",
      "Train Loss: 1.1109, Train Acc: 0.3356, Train Precision: 0.3357, Train Recall: 0.3356, Train  F1: 0.3355,Train ROC AUC: 0.5001704264590695, Train RMSE: 1.1578\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: RAdam \n",
      "Val Loss: 1.1045, Val Acc: 0.3357, Val Precision: 0.1119, Val Recall: 0.3333, Val F1: 0.1676, Val ROC AUC: 0.499911856340936, Val RMSE: 0.8150\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: RAdam Test Loss: 1.1077, Test Acc: 0.3244, Test Precision: 0.1081, Test Recall: 0.3333, Test F1: 0.1633, Test ROC AUC: 0.49999149719983577, Test RMSE: 0.8219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [02:19<00:00, 17.27it/s]\n",
      "100%|| 691/691 [00:38<00:00, 18.17it/s]\n",
      "100%|| 346/346 [00:16<00:00, 20.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.01 \n",
      "Optimizer: LAMB \n",
      "Train Loss: 1.1004, Train Acc: 0.3539, Train Precision: 0.3564, Train Recall: 0.3537, Train  F1: 0.3482,Train ROC AUC: 0.5318598008072684, Train RMSE: 1.0773\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.01\n",
      "Optimizer: LAMB \n",
      "Val Loss: 1.0966, Val Acc: 0.3357, Val Precision: 0.3779, Val Recall: 0.3415, Val F1: 0.2478, Val ROC AUC: 0.5165392764888321, Val RMSE: 1.2823\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.01\n",
      "Optimizer: LAMB Test Loss: 1.0983, Test Acc: 0.3425, Test Precision: 0.3593, Test Recall: 0.3387, Test F1: 0.2482, Test ROC AUC: 0.5125255210672298, Test RMSE: 1.2714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [02:36<00:00, 15.45it/s]\n",
      "100%|| 691/691 [00:36<00:00, 19.09it/s]\n",
      "100%|| 346/346 [00:17<00:00, 19.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.01 \n",
      "Optimizer: AdamW \n",
      "Train Loss: 1.1061, Train Acc: 0.3402, Train Precision: 0.3400, Train Recall: 0.3402, Train  F1: 0.3399,Train ROC AUC: 0.5072211263130332, Train RMSE: 1.1438\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.01\n",
      "Optimizer: AdamW \n",
      "Val Loss: 1.0991, Val Acc: 0.3354, Val Precision: 0.2230, Val Recall: 0.3330, Val F1: 0.1678, Val ROC AUC: 0.49693107569348993, Val RMSE: 0.8162\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.01\n",
      "Optimizer: AdamW Test Loss: 1.1003, Test Acc: 0.3248, Test Precision: 0.2750, Test Recall: 0.3337, Test F1: 0.1642, Test ROC AUC: 0.4993140018530216, Test RMSE: 0.8244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [02:30<00:00, 16.04it/s]\n",
      "100%|| 691/691 [00:40<00:00, 17.16it/s]\n",
      "100%|| 346/346 [00:20<00:00, 16.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.01 \n",
      "Optimizer: SGD \n",
      "Train Loss: 1.1023, Train Acc: 0.3384, Train Precision: 0.3383, Train Recall: 0.3384, Train  F1: 0.3381,Train ROC AUC: 0.5069790265549272, Train RMSE: 1.1370\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.01\n",
      "Optimizer: SGD \n",
      "Val Loss: 1.0985, Val Acc: 0.3379, Val Precision: 0.3057, Val Recall: 0.3355, Val F1: 0.1741, Val ROC AUC: 0.5161830438402765, Val RMSE: 0.8160\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.01\n",
      "Optimizer: SGD Test Loss: 1.0991, Test Acc: 0.3266, Test Precision: 0.3507, Test Recall: 0.3355, Test F1: 0.1691, Test ROC AUC: 0.5253162972970306, Test RMSE: 0.8213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [02:06<00:00, 19.16it/s]\n",
      "100%|| 691/691 [00:25<00:00, 26.85it/s]\n",
      "100%|| 346/346 [00:12<00:00, 27.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.01 \n",
      "Optimizer: RAdam \n",
      "Train Loss: 1.1042, Train Acc: 0.3550, Train Precision: 0.3555, Train Recall: 0.3548, Train  F1: 0.3520,Train ROC AUC: 0.5238191307870083, Train RMSE: 1.1435\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.01\n",
      "Optimizer: RAdam \n",
      "Val Loss: 1.0987, Val Acc: 0.3341, Val Precision: 0.5950, Val Recall: 0.3413, Val F1: 0.1855, Val ROC AUC: 0.5100860267414437, Val RMSE: 1.2918\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.01\n",
      "Optimizer: RAdam Test Loss: 1.0970, Test Acc: 0.3497, Test Precision: 0.6168, Test Recall: 0.3446, Test F1: 0.1935, Test ROC AUC: 0.5117124331839148, Test RMSE: 1.2784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [01:49<00:00, 22.07it/s]\n",
      "100%|| 691/691 [00:23<00:00, 29.56it/s]\n",
      "100%|| 346/346 [00:13<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.001 \n",
      "Optimizer: LAMB \n",
      "Train Loss: 1.0805, Train Acc: 0.3968, Train Precision: 0.3966, Train Recall: 0.3969, Train  F1: 0.3966,Train ROC AUC: 0.5788942549714046, Train RMSE: 1.0997\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.001\n",
      "Optimizer: LAMB \n",
      "Val Loss: 1.0633, Val Acc: 0.4310, Val Precision: 0.4343, Val Recall: 0.4283, Val F1: 0.4104, Val ROC AUC: 0.6213369539640782, Val RMSE: 1.0508\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.001\n",
      "Optimizer: LAMB Test Loss: 1.0560, Test Acc: 0.4471, Test Precision: 0.4568, Test Recall: 0.4483, Test F1: 0.4295, Test ROC AUC: 0.6402112204385372, Test RMSE: 1.0453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [01:42<00:00, 23.65it/s]\n",
      "100%|| 691/691 [00:24<00:00, 27.90it/s]\n",
      "100%|| 346/346 [00:18<00:00, 19.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.001 \n",
      "Optimizer: AdamW \n",
      "Train Loss: 1.0837, Train Acc: 0.4027, Train Precision: 0.4026, Train Recall: 0.4028, Train  F1: 0.4027,Train ROC AUC: 0.5764425104854255, Train RMSE: 1.0931\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.001\n",
      "Optimizer: AdamW \n",
      "Val Loss: 1.0604, Val Acc: 0.4303, Val Precision: 0.4332, Val Recall: 0.4309, Val F1: 0.4293, Val ROC AUC: 0.6208436004235892, Val RMSE: 1.0817\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.001\n",
      "Optimizer: AdamW Test Loss: 1.0532, Test Acc: 0.4395, Test Precision: 0.4423, Test Recall: 0.4387, Test F1: 0.4386, Test ROC AUC: 0.6345390816666536, Test RMSE: 1.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [01:37<00:00, 24.74it/s]\n",
      "100%|| 691/691 [00:26<00:00, 26.55it/s]\n",
      "100%|| 346/346 [00:17<00:00, 19.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.001 \n",
      "Optimizer: SGD \n",
      "Train Loss: 1.0916, Train Acc: 0.3763, Train Precision: 0.3763, Train Recall: 0.3764, Train  F1: 0.3762,Train ROC AUC: 0.5504050736159277, Train RMSE: 1.1220\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.001\n",
      "Optimizer: SGD \n",
      "Val Loss: 1.0803, Val Acc: 0.3999, Val Precision: 0.4181, Val Recall: 0.4026, Val F1: 0.3836, Val ROC AUC: 0.5953129588623661, Val RMSE: 1.1425\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.001\n",
      "Optimizer: SGD Test Loss: 1.0722, Test Acc: 0.4258, Test Precision: 0.4467, Test Recall: 0.4237, Test F1: 0.4109, Test ROC AUC: 0.6154849512111594, Test RMSE: 1.1243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [01:37<00:00, 24.82it/s]\n",
      "100%|| 691/691 [00:32<00:00, 21.19it/s]\n",
      "100%|| 346/346 [00:11<00:00, 29.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.001 \n",
      "Optimizer: RAdam \n",
      "Train Loss: 1.0839, Train Acc: 0.3966, Train Precision: 0.3965, Train Recall: 0.3966, Train  F1: 0.3965,Train ROC AUC: 0.5751289659120008, Train RMSE: 1.0991\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.001\n",
      "Optimizer: RAdam \n",
      "Val Loss: 1.0780, Val Acc: 0.3999, Val Precision: 0.4387, Val Recall: 0.3981, Val F1: 0.3505, Val ROC AUC: 0.6217157931195921, Val RMSE: 0.9137\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.001\n",
      "Optimizer: RAdam Test Loss: 1.0760, Test Acc: 0.4044, Test Precision: 0.4687, Test Recall: 0.4102, Test F1: 0.3584, Test ROC AUC: 0.6325827180670744, Test RMSE: 0.8955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [01:46<00:00, 22.69it/s]\n",
      "100%|| 691/691 [00:29<00:00, 23.79it/s]\n",
      "100%|| 346/346 [00:15<00:00, 22.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.0001 \n",
      "Optimizer: LAMB \n",
      "Train Loss: 1.0940, Train Acc: 0.3697, Train Precision: 0.3718, Train Recall: 0.3695, Train  F1: 0.3677,Train ROC AUC: 0.541354136736867, Train RMSE: 1.1054\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.0001\n",
      "Optimizer: LAMB \n",
      "Val Loss: 1.0864, Val Acc: 0.3946, Val Precision: 0.3967, Val Recall: 0.3934, Val F1: 0.3849, Val ROC AUC: 0.5813343881047596, Val RMSE: 1.1415\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.0001\n",
      "Optimizer: LAMB Test Loss: 1.0830, Test Acc: 0.4153, Test Precision: 0.4179, Test Recall: 0.4143, Test F1: 0.4068, Test ROC AUC: 0.599871698672347, Test RMSE: 1.1197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [01:47<00:00, 22.37it/s]\n",
      "100%|| 691/691 [00:26<00:00, 25.73it/s]\n",
      "100%|| 346/346 [00:12<00:00, 28.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.0001 \n",
      "Optimizer: AdamW \n",
      "Train Loss: 1.0827, Train Acc: 0.3954, Train Precision: 0.3953, Train Recall: 0.3954, Train  F1: 0.3954,Train ROC AUC: 0.5745531366265947, Train RMSE: 1.1003\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.0001\n",
      "Optimizer: AdamW \n",
      "Val Loss: 1.0640, Val Acc: 0.4256, Val Precision: 0.4287, Val Recall: 0.4257, Val F1: 0.4234, Val ROC AUC: 0.6187311804392085, Val RMSE: 1.1057\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.0001\n",
      "Optimizer: AdamW Test Loss: 1.0576, Test Acc: 0.4417, Test Precision: 0.4443, Test Recall: 0.4405, Test F1: 0.4386, Test ROC AUC: 0.6325477133350689, Test RMSE: 1.0980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [01:49<00:00, 22.13it/s]\n",
      "100%|| 691/691 [00:25<00:00, 27.43it/s]\n",
      "100%|| 346/346 [00:11<00:00, 30.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.0001 \n",
      "Optimizer: SGD \n",
      "Train Loss: 1.0975, Train Acc: 0.3521, Train Precision: 0.3519, Train Recall: 0.3521, Train  F1: 0.3519,Train ROC AUC: 0.523181569535176, Train RMSE: 1.1364\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.0001\n",
      "Optimizer: SGD \n",
      "Val Loss: 1.0904, Val Acc: 0.3892, Val Precision: 0.3908, Val Recall: 0.3896, Val F1: 0.3882, Val ROC AUC: 0.5687557133092274, Val RMSE: 1.1074\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.0001\n",
      "Optimizer: SGD Test Loss: 1.0882, Test Acc: 0.3954, Test Precision: 0.3964, Test Recall: 0.3956, Test F1: 0.3939, Test ROC AUC: 0.5823286527063835, Test RMSE: 1.0954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [01:46<00:00, 22.61it/s]\n",
      "100%|| 691/691 [00:24<00:00, 28.76it/s]\n",
      "100%|| 346/346 [00:17<00:00, 19.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.0001 \n",
      "Optimizer: RAdam \n",
      "Train Loss: 1.0877, Train Acc: 0.3828, Train Precision: 0.3837, Train Recall: 0.3828, Train  F1: 0.3818,Train ROC AUC: 0.5610986254402435, Train RMSE: 1.1287\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.0001\n",
      "Optimizer: RAdam \n",
      "Val Loss: 1.0688, Val Acc: 0.4388, Val Precision: 0.4446, Val Recall: 0.4393, Val F1: 0.4368, Val ROC AUC: 0.6168207680858955, Val RMSE: 1.0970\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 0.0001\n",
      "Optimizer: RAdam Test Loss: 1.0613, Test Acc: 0.4526, Test Precision: 0.4570, Test Recall: 0.4512, Test F1: 0.4492, Test ROC AUC: 0.6339922316229826, Test RMSE: 1.0866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [02:10<00:00, 18.53it/s]\n",
      "100%|| 691/691 [00:32<00:00, 20.96it/s]\n",
      "100%|| 346/346 [00:14<00:00, 23.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 1e-05 \n",
      "Optimizer: LAMB \n",
      "Train Loss: 1.1162, Train Acc: 0.3397, Train Precision: 0.3830, Train Recall: 0.3408, Train  F1: 0.2566,Train ROC AUC: 0.5083233818208676, Train RMSE: 1.2804\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 1e-05\n",
      "Optimizer: LAMB \n",
      "Val Loss: 1.1052, Val Acc: 0.3446, Val Precision: 0.2289, Val Recall: 0.3431, Val F1: 0.2614, Val ROC AUC: 0.5205439355423205, Val RMSE: 1.2705\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 1e-05\n",
      "Optimizer: LAMB Test Loss: 1.1014, Test Acc: 0.3787, Test Precision: 0.2607, Test Recall: 0.3740, Test F1: 0.2907, Test ROC AUC: 0.5401228153775323, Test RMSE: 1.2296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [02:04<00:00, 19.36it/s]\n",
      "100%|| 691/691 [00:33<00:00, 20.63it/s]\n",
      "100%|| 346/346 [00:16<00:00, 21.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 1e-05 \n",
      "Optimizer: AdamW \n",
      "Train Loss: 1.0955, Train Acc: 0.3615, Train Precision: 0.3639, Train Recall: 0.3613, Train  F1: 0.3586,Train ROC AUC: 0.533642487676005, Train RMSE: 1.1077\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 1e-05\n",
      "Optimizer: AdamW \n",
      "Val Loss: 1.0880, Val Acc: 0.3935, Val Precision: 0.3978, Val Recall: 0.3946, Val F1: 0.3778, Val ROC AUC: 0.579208759191134, Val RMSE: 1.1547\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 1e-05\n",
      "Optimizer: AdamW Test Loss: 1.0838, Test Acc: 0.4117, Test Precision: 0.4138, Test Recall: 0.4089, Test F1: 0.3954, Test ROC AUC: 0.5984451384579699, Test RMSE: 1.1406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [02:02<00:00, 19.75it/s]\n",
      "100%|| 691/691 [00:30<00:00, 23.00it/s]\n",
      "100%|| 346/346 [00:16<00:00, 21.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 1e-05 \n",
      "Optimizer: SGD \n",
      "Train Loss: 1.1039, Train Acc: 0.3323, Train Precision: 0.3313, Train Recall: 0.3321, Train  F1: 0.3279,Train ROC AUC: 0.49557947685413023, Train RMSE: 1.1876\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 1e-05\n",
      "Optimizer: SGD \n",
      "Val Loss: 1.0999, Val Acc: 0.3305, Val Precision: 0.3322, Val Recall: 0.3315, Val F1: 0.3263, Val ROC AUC: 0.5018523128223151, Val RMSE: 1.2069\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 1e-05\n",
      "Optimizer: SGD Test Loss: 1.1006, Test Acc: 0.3266, Test Precision: 0.3304, Test Recall: 0.3256, Test F1: 0.3232, Test ROC AUC: 0.49180718549092955, Test RMSE: 1.2371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2416/2416 [02:04<00:00, 19.36it/s]\n",
      "100%|| 691/691 [00:41<00:00, 16.54it/s]\n",
      "100%|| 346/346 [00:14<00:00, 23.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 1e-05 \n",
      "Optimizer: RAdam \n",
      "Train Loss: 1.0981, Train Acc: 0.3529, Train Precision: 0.3540, Train Recall: 0.3532, Train  F1: 0.3490,Train ROC AUC: 0.5254616234753626, Train RMSE: 1.1418\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 1e-05\n",
      "Optimizer: RAdam \n",
      "Val Loss: 1.0904, Val Acc: 0.3866, Val Precision: 0.3888, Val Recall: 0.3876, Val F1: 0.3831, Val ROC AUC: 0.5679582169556819, Val RMSE: 1.1290\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 8 \n",
      "Learning Rate: 1e-05\n",
      "Optimizer: RAdam Test Loss: 1.0878, Test Acc: 0.4120, Test Precision: 0.4127, Test Recall: 0.4107, Test F1: 0.4081, Test ROC AUC: 0.5860953985575531, Test RMSE: 1.1119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1208/1208 [01:30<00:00, 13.33it/s]\n",
      "100%|| 346/346 [00:25<00:00, 13.74it/s]\n",
      "100%|| 173/173 [00:12<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.1 \n",
      "Optimizer: LAMB \n",
      "Train Loss: 1.1143, Train Acc: 0.3256, Train Precision: 0.3253, Train Recall: 0.3255, Train  F1: 0.3208,Train ROC AUC: 0.49530398511653795, Train RMSE: 1.1987\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: LAMB \n",
      "Val Loss: 1.1060, Val Acc: 0.3260, Val Precision: 0.2515, Val Recall: 0.3333, Val F1: 0.1649, Val ROC AUC: 0.49222614936958653, Val RMSE: 1.2989\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: LAMB Test Loss: 1.1089, Test Acc: 0.3382, Test Precision: 0.1961, Test Recall: 0.3330, Test F1: 0.1691, Test ROC AUC: 0.4934413358825291, Test RMSE: 1.2939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1208/1208 [01:27<00:00, 13.75it/s]\n",
      "100%|| 346/346 [00:24<00:00, 13.87it/s]\n",
      "100%|| 173/173 [00:12<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.1 \n",
      "Optimizer: AdamW \n",
      "Train Loss: 1.2491, Train Acc: 0.3290, Train Precision: 0.3289, Train Recall: 0.3289, Train  F1: 0.3289,Train ROC AUC: 0.49766461144559454, Train RMSE: 1.1603\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: AdamW \n",
      "Val Loss: 1.0999, Val Acc: 0.3383, Val Precision: 0.1128, Val Recall: 0.3333, Val F1: 0.1685, Val ROC AUC: 0.5, Val RMSE: 1.2805\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: AdamW Test Loss: 1.1000, Test Acc: 0.3371, Test Precision: 0.1124, Test Recall: 0.3333, Test F1: 0.1681, Test ROC AUC: 0.5, Test RMSE: 1.2956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1208/1208 [01:26<00:00, 13.92it/s]\n",
      "100%|| 346/346 [00:25<00:00, 13.77it/s]\n",
      "100%|| 173/173 [00:12<00:00, 14.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.1 \n",
      "Optimizer: SGD \n",
      "Train Loss: 1.1128, Train Acc: 0.3314, Train Precision: 0.3312, Train Recall: 0.3312, Train  F1: 0.3302,Train ROC AUC: 0.4985383773424646, Train RMSE: 1.1646\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: SGD \n",
      "Val Loss: 1.1063, Val Acc: 0.3260, Val Precision: 0.1087, Val Recall: 0.3333, Val F1: 0.1639, Val ROC AUC: 0.49991032923451534, Val RMSE: 1.2996\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: SGD Test Loss: 1.1058, Test Acc: 0.3385, Test Precision: 0.1128, Test Recall: 0.3333, Test F1: 0.1686, Test ROC AUC: 0.4994542299020028, Test RMSE: 1.2933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1208/1208 [01:27<00:00, 13.87it/s]\n",
      "100%|| 346/346 [00:25<00:00, 13.64it/s]\n",
      "100%|| 173/173 [00:12<00:00, 13.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.1 \n",
      "Optimizer: RAdam \n",
      "Train Loss: 1.1106, Train Acc: 0.3376, Train Precision: 0.3375, Train Recall: 0.3376, Train  F1: 0.3375,Train ROC AUC: 0.5032096698113904, Train RMSE: 1.1497\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: RAdam \n",
      "Val Loss: 1.0996, Val Acc: 0.3383, Val Precision: 0.1128, Val Recall: 0.3333, Val F1: 0.1685, Val ROC AUC: 0.5002720978477329, Val RMSE: 1.2805\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.1\n",
      "Optimizer: RAdam Test Loss: 1.0994, Test Acc: 0.3371, Test Precision: 0.1124, Test Recall: 0.3333, Test F1: 0.1681, Test ROC AUC: 0.4992585965668497, Test RMSE: 1.2956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1208/1208 [01:28<00:00, 13.63it/s]\n",
      "100%|| 346/346 [00:24<00:00, 14.06it/s]\n",
      "100%|| 173/173 [00:12<00:00, 14.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.01 \n",
      "Optimizer: LAMB \n",
      "Train Loss: 1.0886, Train Acc: 0.3889, Train Precision: 0.3891, Train Recall: 0.3889, Train  F1: 0.3890,Train ROC AUC: 0.5678685122969372, Train RMSE: 1.1112\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.01\n",
      "Optimizer: LAMB \n",
      "Val Loss: 1.0728, Val Acc: 0.4138, Val Precision: 0.4388, Val Recall: 0.4123, Val F1: 0.3839, Val ROC AUC: 0.6108231928489428, Val RMSE: 1.1582\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.01\n",
      "Optimizer: LAMB Test Loss: 1.0681, Test Acc: 0.4337, Test Precision: 0.4557, Test Recall: 0.4308, Test F1: 0.4020, Test ROC AUC: 0.6226645013256277, Test RMSE: 1.1385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1208/1208 [01:17<00:00, 15.55it/s]\n",
      "100%|| 346/346 [00:25<00:00, 13.31it/s]\n",
      "100%|| 173/173 [00:13<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Values After Training-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.01 \n",
      "Optimizer: AdamW \n",
      "Train Loss: 1.1048, Train Acc: 0.3419, Train Precision: 0.3416, Train Recall: 0.3421, Train  F1: 0.3411,Train ROC AUC: 0.5127005673645104, Train RMSE: 1.1449\n",
      "\n",
      "\n",
      "-----------Values After Validation-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.01\n",
      "Optimizer: AdamW \n",
      "Val Loss: 1.0989, Val Acc: 0.3359, Val Precision: 0.5786, Val Recall: 0.3335, Val F1: 0.1686, Val ROC AUC: 0.5034309147102497, Val RMSE: 0.8152\n",
      "\n",
      "\n",
      "-----------Values After Testing-----------\n",
      "\n",
      "Epoch: [2/2] \n",
      "Batch Size: 16 \n",
      "Learning Rate: 0.01\n",
      "Optimizer: AdamW Test Loss: 1.0998, Test Acc: 0.3248, Test Precision: 0.4415, Test Recall: 0.3337, Test F1: 0.1647, Test ROC AUC: 0.5048845735374067, Test RMSE: 0.8224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1208/1208 [01:28<00:00, 13.62it/s]\n",
      "100%|| 346/346 [00:24<00:00, 13.89it/s]\n",
      "  0%|          | 0/173 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../skintypepatches 128x128\\\\dry\\\\dry_5717c37ca77c9c18fc93_jpg.rf.a3e6fd0dc8d8d8d630e5182177d53e24_patch_2431.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 170\u001b[39m\n\u001b[32m    168\u001b[39m train_loss, train_acc, train_precision, train_recall, train_f1, train_roc_auc, train_rmse = train_model(model, train_loader, optimizer, criterion, device)\n\u001b[32m    169\u001b[39m val_loss, val_acc, val_precision, val_recall, val_f1, val_roc_auc, val_rmse = evaluate_model(model, val_loader, criterion, device)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m test_loss, test_acc, test_precision, test_recall, test_f1, test_roc_auc, test_rmse = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m----------Values After Training-----------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBatch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLearning Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    174\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOptimizer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    175\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_recall\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train  F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain ROC AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_roc_auc\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mtrain_roc_auc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, loader, criterion, device)\u001b[39m\n\u001b[32m    135\u001b[39m all_labels = []\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03m    index (int): Index\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    242\u001b[39m \u001b[33;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m path, target = \u001b[38;5;28mself\u001b[39m.samples[index]\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    247\u001b[39m     sample = \u001b[38;5;28mself\u001b[39m.transform(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[39m, in \u001b[36mdefault_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:262\u001b[39m, in \u001b[36mpil_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> Image.Image:\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    263\u001b[39m         img = Image.open(f)\n\u001b[32m    264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m img.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../skintypepatches 128x128\\\\dry\\\\dry_5717c37ca77c9c18fc93_jpg.rf.a3e6fd0dc8d8d8d630e5182177d53e24_patch_2431.jpg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, root_mean_squared_error\n",
    "from torch_optimizer import Lamb  \n",
    "import clip\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import SGD\n",
    "from pytorch_lamb import Lamb\n",
    "from torch_optimizer import RAdam\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_sizes = [8,16,32,64]\n",
    "learning_rates = [0.1,0.01,0.001,0.0001,0.00001]\n",
    "optimizers_list = ['LAMB', 'AdamW', 'SGD', 'RAdam']\n",
    "num_classes=3\n",
    "total_epochs = 2\n",
    "start=1\n",
    "step=1\n",
    "\n",
    "# Load CLIP model and preprocessing\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                         (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "data_dir = \"../skintypepatches 128x128\"\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "print(\"Classes:\", dataset.classes)\n",
    "\n",
    "# Data Splitting\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Freeze CLIP vision encoder\n",
    "for param in clip_model.visual.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Model definition\n",
    "class CLIPSkinClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=3):\n",
    "        super(CLIPSkinClassifier, self).__init__()\n",
    "        self.encoder = clip_model.visual\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def get_optimizer(optimizer_name, model_params, lr,weight_decay=0.001):\n",
    "    if optimizer_name == 'AdamW':\n",
    "        return AdamW(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'LAMB':\n",
    "        return Lamb(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        return SGD(model_params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'RAdam':\n",
    "        return RAdam(model_params, lr=lr, weight_decay=weight_decay)\n",
    "# Metric helper\n",
    "def compute_metrics(outputs, labels):\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(labels, preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(np.eye(3)[labels], F.softmax(outputs, dim=1).cpu().detach().numpy(), multi_class='ovr')\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    rmse = root_mean_squared_error(labels, preds)\n",
    "    return acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Train loop\n",
    "def train_model(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in tqdm(loader):\n",
    "        images, labels = images.to(device).float(), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_outputs.append(outputs.detach())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    outputs_cat = torch.cat(all_outputs)\n",
    "    labels_cat = torch.cat(all_labels)\n",
    "    acc, precision, recall, f1, roc_auc, rmse = compute_metrics(outputs_cat, labels_cat)\n",
    "    return total_loss / len(loader), acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Eval loop\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader):\n",
    "            images, labels = images.to(device).float(), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_outputs.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    outputs_cat = torch.cat(all_outputs)\n",
    "    labels_cat = torch.cat(all_labels)\n",
    "    acc, precision, recall, f1, roc_auc, rmse = compute_metrics(outputs_cat, labels_cat)\n",
    "    return total_loss / len(loader), acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "# Training loop with logging\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for optimizer_name in optimizers_list:\n",
    "            \n",
    "            model = CLIPSkinClassifier(clip_model, num_classes=num_classes).to(device).float()\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = get_optimizer(optimizer_name,model.parameters(), lr=lr)\n",
    "            \n",
    "            \n",
    "            for epoch in range(start,total_epochs,step):\n",
    "                train_loss, train_acc, train_precision, train_recall, train_f1, train_roc_auc, train_rmse = train_model(model, train_loader, optimizer, criterion, device)\n",
    "                val_loss, val_acc, val_precision, val_recall, val_f1, val_roc_auc, val_rmse = evaluate_model(model, val_loader, criterion, device)\n",
    "                test_loss, test_acc, test_precision, test_recall, test_f1, test_roc_auc, test_rmse = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "                print(\"----------Values After Training-----------\")\n",
    "                print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\",\n",
    "                    f\"\\nOptimizer: {optimizer_name} \\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                        f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train  F1: {train_f1:.4f},\"\n",
    "                        f\"Train ROC AUC: {train_roc_auc if train_roc_auc is not None else 'N/A'}, Train RMSE: {train_rmse:.4f}\")\n",
    "\n",
    "                print(\"\\n\\n-----------Values After Validation-----------\")\n",
    "                print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "                    f\"\\nOptimizer: {optimizer_name} \\nVal Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "                            f\"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}, Val ROC AUC: {val_roc_auc if train_roc_auc is not None else 'N/A'}, \"\n",
    "                            f\"Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "                print(\"\\n\\n-----------Values After Testing-----------\")\n",
    "                print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "                    f\"\\nOptimizer: {optimizer_name} Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, \"\n",
    "                            f\"Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}, Test ROC AUC: {test_roc_auc if train_roc_auc is not None else 'N/A'}, Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "                # Save results to CSV\n",
    "                overall_result = {\n",
    "                    'Epoch': epoch + 1,\n",
    "                    'Batch Size': batch_size,\n",
    "                    'Learning Rate': lr,\n",
    "                    'Optimizer': optimizer_name,\n",
    "\n",
    "                    'Train Loss': round(train_loss, 4),\n",
    "                    'Test Loss': round(test_loss, 4),\n",
    "                    'Val Loss': round(val_loss, 4),\n",
    "\n",
    "                    'Train Acc': round(train_acc, 4),\n",
    "                    'Test Acc': round(test_acc, 4),\n",
    "                    'Val Acc': round(val_acc, 4),\n",
    "\n",
    "                    'Train Precision': round(train_precision, 4),\n",
    "                    'Test Precision': round(test_precision, 4),\n",
    "                    'Val Precision': round(val_precision, 4),\n",
    "\n",
    "                    'Train Recall': round(train_recall, 4),\n",
    "                    'Test Recall': round(test_recall, 4),\n",
    "                    'Val Recall': round(val_recall, 4),\n",
    "\n",
    "                    'Train F1 Score': round(train_f1, 4),\n",
    "                    'Test F1 Score': round(test_f1, 4),\n",
    "                    'Val F1 Score': round(val_f1, 4),\n",
    "\n",
    "                    'Train ROC AUC': round(train_roc_auc, 4) if train_roc_auc is not None else None,\n",
    "                    'Test ROC AUC': round(test_roc_auc, 4) if test_roc_auc is not None else None,\n",
    "                    'Val ROC AUC': round(val_roc_auc, 4) if val_roc_auc is not None else None,\n",
    "\n",
    "                    'Train RMSE': round(train_rmse, 4),\n",
    "                    'Test RMSE': round(test_rmse, 4),\n",
    "                    'Val RMSE': round(val_rmse, 4)\n",
    "                }\n",
    "\n",
    "                overall_result_file = 'final testing.csv'\n",
    "                if not os.path.isfile(overall_result_file):\n",
    "                    pd.DataFrame([overall_result]).to_csv(overall_result_file, index=False)\n",
    "                else:\n",
    "                    pd.DataFrame([overall_result]).to_csv(overall_result_file, mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CLIP model\n",
    "clip_model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model.eval()  # We won't fine-tune CLIP itself\n",
    "\n",
    "# Dataset path\n",
    "data_dir = '../stage1patches'\n",
    "\n",
    "# Add slight augmentation to training set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    preprocess_clip,\n",
    "])\n",
    "\n",
    "# Use only CLIP's transform for validation\n",
    "val_transform = preprocess_clip\n",
    "\n",
    "# Load dataset\n",
    "full_dataset = datasets.ImageFolder(data_dir)\n",
    "class_names = full_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# Manual split with transforms applied\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_data.dataset.transform = train_transform\n",
    "val_data.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Feature extractor from CLIP (used inside training loop)\n",
    "def extract_features(model, images):\n",
    "    with torch.no_grad():\n",
    "        return model.encode_image(images).float()\n",
    "\n",
    "# Improved classifier\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=512, num_classes=3):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Initialize classifier\n",
    "model = MLPClassifier(input_dim=512, num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Loss, optimizer, scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "best_acc = 0.0\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        features = extract_features(clip_model, images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = extract_features(clip_model, images)\n",
    "            outputs = model(features)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    scheduler.step(acc)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f} - Val Acc: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    if acc >= 0.96:\n",
    "        print(\" Target accuracy of 96% reached! Stopping early.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
