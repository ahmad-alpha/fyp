{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a69f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install matplotlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f012259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import clip  # OpenAI's CLIP\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CLIP model and preprocessing\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Custom transform to 128x128 + CLIP normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                         (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# Replace this with your dataset path\n",
    "data_dir = \"C:/Users/shazi/OneDrive/Desktop/VS Code/fyp/skintypepatches 128x128\"  # example: \"dataset/\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Print class names\n",
    "print(\"Classes:\", dataset.classes)\n",
    "\n",
    "# Freeze CLIP vision encoder layers\n",
    "for param in clip_model.visual.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Build the model using CLIP ViT encoder\n",
    "class CLIPSkinClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=3):\n",
    "        super(CLIPSkinClassifier, self).__init__()\n",
    "        self.encoder = clip_model.visual\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "model = CLIPSkinClassifier(clip_model, num_classes=3).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for images, labels in loop:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            loop.set_postfix(loss=loss.item(), acc=100. * correct / total)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss={running_loss/len(dataloader):.4f}, Accuracy={100. * correct/total:.2f}%\")\n",
    "\n",
    "# Train\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf53190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import clip  # OpenAI's CLIP\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CLIP model and preprocessing\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Custom transform to 128x128 + CLIP normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                         (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# Your dataset path\n",
    "data_dir = \"C:/Users/shazi/OneDrive/Desktop/VS Code/fyp/skintypepatches 128x128\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Print class names\n",
    "print(\"Classes:\", dataset.classes)\n",
    "\n",
    "# Freeze CLIP vision encoder layers\n",
    "for param in clip_model.visual.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Build model with CLIP ViT encoder\n",
    "class CLIPSkinClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=3):\n",
    "        super(CLIPSkinClassifier, self).__init__()\n",
    "        self.encoder = clip_model.visual\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.half()  # Ensure input is float16\n",
    "        features = self.encoder(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "# Initialize model and convert to half precision\n",
    "model = CLIPSkinClassifier(clip_model, num_classes=3).to(device).half()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for images, labels in loop:\n",
    "            images = images.to(device).half()  # Match input type to model (float16)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            loop.set_postfix(loss=loss.item(), acc=100. * correct / total)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss={running_loss/len(dataloader):.4f}, Accuracy={100. * correct/total:.2f}%\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import clip  # OpenAI's CLIP\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CLIP model and preprocessing\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Custom transform to 224x224 + CLIP normalization (corrected size)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224 for CLIP compatibility\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                         (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# Replace this with your dataset path\n",
    "data_dir = r\"C:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\fyp\\skintypepatches 128x128\"  # example: \"dataset/\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Print class names\n",
    "print(\"Classes:\", dataset.classes)\n",
    "\n",
    "# Freeze CLIP vision encoder layers\n",
    "for param in clip_model.visual.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Build the model using CLIP ViT encoder\n",
    "class CLIPSkinClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=3):\n",
    "        super(CLIPSkinClassifier, self).__init__()\n",
    "        self.encoder = clip_model.visual\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "model = CLIPSkinClassifier(clip_model, num_classes=3).to(device).float()  # Ensure model is in float32\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for images, labels in tqdm(loop):\n",
    "            images, labels = images.to(device).float(), labels.to(device)  # Ensure input is in float32\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            loop.set_postfix(loss=loss.item(), acc=100. * correct / total)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss={running_loss/len(dataloader):.4f}, Accuracy={100. * correct/total:.2f}%\")\n",
    "\n",
    "# Train\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb601909",
   "metadata": {},
   "source": [
    "DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63650df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Classes: ['dryPatches', 'normalPatch', 'oilyPatch']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 602.9782 - Val Acc: 0.4838\n",
      "Epoch 2/100 - Loss: 571.0641 - Val Acc: 0.5034\n",
      "Epoch 3/100 - Loss: 553.4542 - Val Acc: 0.5153\n",
      "Epoch 4/100 - Loss: 541.4493 - Val Acc: 0.5281\n",
      "Epoch 5/100 - Loss: 531.4673 - Val Acc: 0.5333\n",
      "Epoch 6/100 - Loss: 517.9318 - Val Acc: 0.5533\n",
      "Epoch 7/100 - Loss: 507.6806 - Val Acc: 0.5596\n",
      "Epoch 8/100 - Loss: 497.1901 - Val Acc: 0.5764\n",
      "Epoch 9/100 - Loss: 488.4810 - Val Acc: 0.5755\n",
      "Epoch 10/100 - Loss: 478.9524 - Val Acc: 0.5856\n",
      "Epoch 11/100 - Loss: 469.0457 - Val Acc: 0.5919\n",
      "Epoch 12/100 - Loss: 458.9447 - Val Acc: 0.6022\n",
      "Epoch 13/100 - Loss: 449.1411 - Val Acc: 0.6090\n",
      "Epoch 14/100 - Loss: 442.1322 - Val Acc: 0.6151\n",
      "Epoch 15/100 - Loss: 434.6999 - Val Acc: 0.6265\n",
      "Epoch 16/100 - Loss: 424.5018 - Val Acc: 0.6249\n",
      "Epoch 17/100 - Loss: 417.5149 - Val Acc: 0.6324\n",
      "Epoch 18/100 - Loss: 412.0345 - Val Acc: 0.6384\n",
      "Epoch 19/100 - Loss: 404.4299 - Val Acc: 0.6445\n",
      "Epoch 20/100 - Loss: 393.3150 - Val Acc: 0.6508\n",
      "Epoch 21/100 - Loss: 384.9321 - Val Acc: 0.6643\n",
      "Epoch 22/100 - Loss: 382.1463 - Val Acc: 0.6530\n",
      "Epoch 23/100 - Loss: 377.9726 - Val Acc: 0.6602\n",
      "Epoch 24/100 - Loss: 367.4499 - Val Acc: 0.6744\n",
      "Epoch 25/100 - Loss: 361.4983 - Val Acc: 0.6730\n",
      "Epoch 26/100 - Loss: 356.8691 - Val Acc: 0.6679\n",
      "Epoch 27/100 - Loss: 351.1696 - Val Acc: 0.6748\n",
      "Epoch 28/100 - Loss: 348.9632 - Val Acc: 0.6811\n",
      "Epoch 29/100 - Loss: 339.8988 - Val Acc: 0.6831\n",
      "Epoch 30/100 - Loss: 334.6416 - Val Acc: 0.6919\n",
      "Epoch 31/100 - Loss: 330.3704 - Val Acc: 0.6966\n",
      "Epoch 32/100 - Loss: 324.9862 - Val Acc: 0.6939\n",
      "Epoch 33/100 - Loss: 319.1020 - Val Acc: 0.6933\n",
      "Epoch 34/100 - Loss: 314.8641 - Val Acc: 0.6930\n",
      "Epoch 35/100 - Loss: 312.7380 - Val Acc: 0.7040\n",
      "Epoch 36/100 - Loss: 309.2353 - Val Acc: 0.7052\n",
      "Epoch 37/100 - Loss: 306.8873 - Val Acc: 0.7038\n",
      "Epoch 38/100 - Loss: 297.1041 - Val Acc: 0.7020\n",
      "Epoch 39/100 - Loss: 291.3585 - Val Acc: 0.7097\n",
      "Epoch 40/100 - Loss: 293.2047 - Val Acc: 0.7148\n",
      "Epoch 41/100 - Loss: 287.5157 - Val Acc: 0.7121\n",
      "Epoch 42/100 - Loss: 286.1341 - Val Acc: 0.7209\n",
      "Epoch 43/100 - Loss: 279.0517 - Val Acc: 0.7153\n",
      "Epoch 44/100 - Loss: 276.6626 - Val Acc: 0.7157\n",
      "Epoch 45/100 - Loss: 269.6760 - Val Acc: 0.7128\n",
      "Epoch 46/100 - Loss: 267.2000 - Val Acc: 0.7182\n",
      "Epoch 47/100 - Loss: 268.8176 - Val Acc: 0.7234\n",
      "Epoch 48/100 - Loss: 263.6186 - Val Acc: 0.7258\n",
      "Epoch 49/100 - Loss: 255.6243 - Val Acc: 0.7283\n",
      "Epoch 50/100 - Loss: 261.2571 - Val Acc: 0.7236\n",
      "Epoch 51/100 - Loss: 248.5798 - Val Acc: 0.7243\n",
      "Epoch 52/100 - Loss: 246.5543 - Val Acc: 0.7209\n",
      "Epoch 53/100 - Loss: 254.9502 - Val Acc: 0.7362\n",
      "Epoch 54/100 - Loss: 252.5412 - Val Acc: 0.7213\n",
      "Epoch 55/100 - Loss: 240.4822 - Val Acc: 0.7274\n",
      "Epoch 56/100 - Loss: 242.5879 - Val Acc: 0.7146\n",
      "Epoch 57/100 - Loss: 239.3726 - Val Acc: 0.7263\n",
      "Epoch 58/100 - Loss: 233.4324 - Val Acc: 0.7303\n",
      "Epoch 59/100 - Loss: 236.0738 - Val Acc: 0.7263\n",
      "Epoch 60/100 - Loss: 212.6654 - Val Acc: 0.7425\n",
      "Epoch 61/100 - Loss: 205.8106 - Val Acc: 0.7400\n",
      "Epoch 62/100 - Loss: 208.0376 - Val Acc: 0.7463\n",
      "Epoch 63/100 - Loss: 200.4939 - Val Acc: 0.7375\n",
      "Epoch 64/100 - Loss: 202.9420 - Val Acc: 0.7369\n",
      "Epoch 65/100 - Loss: 198.2235 - Val Acc: 0.7463\n",
      "Epoch 66/100 - Loss: 193.2480 - Val Acc: 0.7476\n",
      "Epoch 67/100 - Loss: 196.3730 - Val Acc: 0.7485\n",
      "Epoch 68/100 - Loss: 192.0528 - Val Acc: 0.7510\n",
      "Epoch 69/100 - Loss: 191.7645 - Val Acc: 0.7443\n",
      "Epoch 70/100 - Loss: 196.9544 - Val Acc: 0.7467\n",
      "Epoch 71/100 - Loss: 193.7490 - Val Acc: 0.7467\n",
      "Epoch 72/100 - Loss: 195.9625 - Val Acc: 0.7535\n",
      "Epoch 73/100 - Loss: 189.7660 - Val Acc: 0.7418\n",
      "Epoch 74/100 - Loss: 192.5052 - Val Acc: 0.7461\n",
      "Epoch 75/100 - Loss: 195.4748 - Val Acc: 0.7506\n",
      "Epoch 76/100 - Loss: 191.9370 - Val Acc: 0.7470\n",
      "Epoch 77/100 - Loss: 186.3090 - Val Acc: 0.7452\n",
      "Epoch 78/100 - Loss: 190.8195 - Val Acc: 0.7524\n",
      "Epoch 79/100 - Loss: 187.9270 - Val Acc: 0.7530\n",
      "Epoch 80/100 - Loss: 187.4769 - Val Acc: 0.7488\n",
      "Epoch 81/100 - Loss: 186.7980 - Val Acc: 0.7510\n",
      "Epoch 82/100 - Loss: 189.0219 - Val Acc: 0.7542\n",
      "Epoch 83/100 - Loss: 182.7052 - Val Acc: 0.7443\n",
      "Epoch 84/100 - Loss: 185.2579 - Val Acc: 0.7425\n",
      "Epoch 85/100 - Loss: 183.9942 - Val Acc: 0.7524\n",
      "Epoch 86/100 - Loss: 186.4885 - Val Acc: 0.7546\n",
      "Epoch 87/100 - Loss: 181.9783 - Val Acc: 0.7544\n",
      "Epoch 88/100 - Loss: 184.7786 - Val Acc: 0.7458\n",
      "Epoch 89/100 - Loss: 184.7641 - Val Acc: 0.7393\n",
      "Epoch 90/100 - Loss: 186.3859 - Val Acc: 0.7519\n",
      "Epoch 91/100 - Loss: 185.0184 - Val Acc: 0.7528\n",
      "Epoch 92/100 - Loss: 179.1217 - Val Acc: 0.7519\n",
      "Epoch 93/100 - Loss: 181.9341 - Val Acc: 0.7539\n",
      "Epoch 94/100 - Loss: 190.5543 - Val Acc: 0.7539\n",
      "Epoch 95/100 - Loss: 186.8233 - Val Acc: 0.7533\n",
      "Epoch 96/100 - Loss: 188.6770 - Val Acc: 0.7497\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py\", line 245, in __getitem__\n    sample = self.loader(path)\n             ^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py\", line 284, in default_loader\n    return pil_loader(path)\n           ^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py\", line 262, in pil_loader\n    with open(path, \"rb\") as f:\n         ^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'stage1patches\\\\normalPatch\\\\normal (147)_patch_4959.jpg'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     99\u001b[39m correct = total = \u001b[32m0\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1480\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1478\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task_info[idx]\n\u001b[32m   1479\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1480\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1505\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1503\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1504\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1505\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\_utils.py:733\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py\", line 245, in __getitem__\n    sample = self.loader(path)\n             ^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py\", line 284, in default_loader\n    return pil_loader(path)\n           ^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py\", line 262, in pil_loader\n    with open(path, \"rb\") as f:\n         ^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'stage1patches\\\\normalPatch\\\\normal (147)_patch_4959.jpg'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CLIP model\n",
    "clip_model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model.eval()  # We won't fine-tune CLIP itself\n",
    "\n",
    "# Dataset path\n",
    "data_dir = 'stage1patches'\n",
    "\n",
    "# Add slight augmentation to training set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    preprocess_clip,\n",
    "])\n",
    "\n",
    "# Use only CLIP's transform for validation\n",
    "val_transform = preprocess_clip\n",
    "\n",
    "# Load dataset\n",
    "full_dataset = datasets.ImageFolder(data_dir)\n",
    "class_names = full_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# Manual split with transforms applied\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_data.dataset.transform = train_transform\n",
    "val_data.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Feature extractor from CLIP (used inside training loop)\n",
    "def extract_features(model, images):\n",
    "    with torch.no_grad():\n",
    "        return model.encode_image(images).float()\n",
    "\n",
    "# Improved classifier\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=512, num_classes=3):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Initialize classifier\n",
    "model = MLPClassifier(input_dim=512, num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Loss, optimizer, scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "best_acc = 0.0\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        features = extract_features(clip_model, images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = extract_features(clip_model, images)\n",
    "            outputs = model(features)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    scheduler.step(acc)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f} - Val Acc: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    if acc >= 0.96:\n",
    "        print(\"ðŸŽ¯ Target accuracy of 96% reached! Stopping early.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cef8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
